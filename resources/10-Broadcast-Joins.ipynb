{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"name":"Intro To DF Part 5","notebookId":10796,"colab":{"name":"Copy of 10-Broadcast-Joins.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-9uS9SeUW6q6"},"source":["# Broadcast Joins\n","\n","**Technical Accomplishments:**\n","* Introduce the concept of Broadcast Joins"]},{"cell_type":"markdown","metadata":{"id":"YYsnwVUxu8uw"},"source":["In this exercise you will:<br>\r\n","Learn about the concepts of Broadcast Joins\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"LwT0sf8bW6rE"},"source":["## The Data Source\n","\n","This data uses the **Pageviews By Seconds** data set."]},{"cell_type":"code","metadata":{"id":"bI-Ka6esW6rF"},"source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","\n","# I've already gone through the exercise to determine\n","# how many partitions I want and in this case it is...\n","partitions = 8\n","\n","# Make sure wide operations don't repartition to 200\n","spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCpAxAo4W6rG"},"source":["# The directory containing our parquet files.\n","parquetFile = \"data/pageviews_by_second.parquet/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gv4yEyZFW6rG","outputId":"3f60d13f-2488-4b9a-a2a4-8e966fd4ca86"},"source":["# Create our initial DataFrame. We can let it infer the \n","# schema because the cost for parquet files is really low.\n","pageviewsDF = (spark.read\n","  .option(\"inferSchema\", \"true\")                # The default, but not costly w/Parquet\n","  .parquet(parquetFile)                         # Read the data in\n","  .repartition(partitions)                      # From 7 >>> 8 partitions\n","  .withColumnRenamed(\"timestamp\", \"capturedAt\") # rename and convert to timestamp datatype\n","  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\n","  .orderBy( col(\"capturedAt\"), col(\"site\") )    # sort our records\n","  .cache()                                      # Cache the expensive operation\n",")\n","# materialize the cache\n","pageviewsDF.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7200000"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"SJJMhGVrW6rI"},"source":["## Broadcast Joins\n","\n","If you saw the section on UDFs carefully, you can say that we can **aggregate by the Day-Of-Week**.\n","\n","We will **first use a UDF** only to discover that either there was a built in function to do the exact same thing or not.\n","\n","We also saw that **Monday had more data** than that of any other day of the week.\n","\n","We will then forked the `DataFrame` in such a way so as it can be compared with the **Mobile Requests to Desktop Requests**.\n","\n","Next, we will **join those to `DataFrames`** into one, so that we could easily compare the two sets of the data.\n","\n","We you can say that the problem with the data **has nothing to do with Mobile vs Desktop**.\n","\n","So we don't need that type of join (two ~large `DataFrames`)\n","\n","However, what if we wanted to **reproduce our first exercise** (counts per day-of-week)\n","* without a UDF\n","* with a lookup table for the day-of-week\n","* with a join between the pageviews and the lookup table\n","\n","What's different about this example is that we are **joining a big `DataFrame` to a small `DataFrame`**.\n","\n","In this scenario, Spark can optimize the join and **avoid the expensive shuffle** with a **Broadcast Join**.\n","\n","Let's start with two `DataFrames`\n","* The first we will derive from our original `DataFrame`. In this case, we will use a simple number for the day-of-week.\n","* The second `DataFrame` will map that number (1-7) to the labels **Monday**, **Tue**, **W**, or whatever...\n","\n","Let's take a look at our first `DataFrame`."]},{"cell_type":"code","metadata":{"id":"_NZRjVEnW6rJ","outputId":"399c5845-bbc2-4d47-c48d-ca045987466c"},"source":["columnTrans = date_format(col(\"capturedAt\"), \"u\").alias(\"dow\")\n","\n","pageviewsWithDowDF = (pageviewsDF\n","    .withColumn(\"dow\", columnTrans)  # Add the column dow\n",")\n","(pageviewsWithDowDF\n","  .cache()                           # mark the data as cached\n","  .count()                           # materialize the cache\n",")\n","pageviewsWithDowDF.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-------+--------+---+\n","|         capturedAt|   site|requests|dow|\n","+-------------------+-------+--------+---+\n","|2015-03-16 00:00:00|desktop|    2343|  1|\n","|2015-03-16 00:00:00| mobile|    1628|  1|\n","|2015-03-16 00:00:01|desktop|    2382|  1|\n","|2015-03-16 00:00:01| mobile|    1636|  1|\n","|2015-03-16 00:00:02|desktop|    2546|  1|\n","|2015-03-16 00:00:02| mobile|    1619|  1|\n","|2015-03-16 00:00:03|desktop|    2402|  1|\n","|2015-03-16 00:00:03| mobile|    1776|  1|\n","|2015-03-16 00:00:04|desktop|    2370|  1|\n","|2015-03-16 00:00:04| mobile|    1716|  1|\n","|2015-03-16 00:00:05|desktop|    2417|  1|\n","|2015-03-16 00:00:05| mobile|    1721|  1|\n","|2015-03-16 00:00:06|desktop|    2318|  1|\n","|2015-03-16 00:00:06| mobile|    1695|  1|\n","|2015-03-16 00:00:07|desktop|    2580|  1|\n","|2015-03-16 00:00:07| mobile|    1630|  1|\n","|2015-03-16 00:00:08|desktop|    2545|  1|\n","|2015-03-16 00:00:08| mobile|    1731|  1|\n","|2015-03-16 00:00:09|desktop|    2366|  1|\n","|2015-03-16 00:00:09| mobile|    1664|  1|\n","+-------------------+-------+--------+---+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v8U1fOw8W6rJ"},"source":["All we did here is that, add one column **dow** which has the value **1** for **Monday**, **2** for **Tuesday**, etc.\n","\n","Next, we are going to load a mapping of 1, 2, 3, etc. to Mon, Tue, Wed, etc from a **REALLY** small `DataFrame`."]},{"cell_type":"code","metadata":{"id":"4k8EJY2fW6rK","outputId":"0b42c689-6af2-4114-e04d-6e72c33ca990"},"source":["labelsDF = spark.read.parquet(\"data/day-of-week\")\n","\n","display(labelsDF) # view our labels"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["DataFrame[dow: int, longName: string, abbreviated: string, shortName: string]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"KIr-r9dVW6rK"},"source":["Now that we have the two `DataFrames`.\n","\n","Finally, we can execute a join between the two `DataFrames`"]},{"cell_type":"code","metadata":{"id":"m8cmQXtCW6rK","outputId":"418f0933-d991-44fc-d222-446ac31d2c0f"},"source":["joinedDowDF = (pageviewsWithDowDF\n","  .join(labelsDF, pageviewsWithDowDF[\"dow\"] == labelsDF[\"dow\"])\n","  .drop( pageviewsWithDowDF[\"dow\"] )\n",")\n","joinedDowDF.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+-------+--------+---+--------+-----------+---------+\n","|         capturedAt|   site|requests|dow|longName|abbreviated|shortName|\n","+-------------------+-------+--------+---+--------+-----------+---------+\n","|2015-03-16 00:00:00|desktop|    2343|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:00| mobile|    1628|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:01|desktop|    2382|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:01| mobile|    1636|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:02|desktop|    2546|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:02| mobile|    1619|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:03|desktop|    2402|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:03| mobile|    1776|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:04|desktop|    2370|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:04| mobile|    1716|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:05|desktop|    2417|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:05| mobile|    1721|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:06|desktop|    2318|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:06| mobile|    1695|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:07|desktop|    2580|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:07| mobile|    1630|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:08|desktop|    2545|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:08| mobile|    1731|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:09|desktop|    2366|  1|  Monday|        Mon|        M|\n","|2015-03-16 00:00:09| mobile|    1664|  1|  Monday|        Mon|        M|\n","+-------------------+-------+--------+---+--------+-----------+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"elwwG4ArW6rL"},"source":["Now, that the data is joined, we can easily aggregate by any (or all) of the various labels which represents the day-of-week.\n","\n","Notice that we are not losing the numerical **dow** column which we can use to sort.\n","\n","And when we will group this, you can group by any one of the labels."]},{"cell_type":"code","metadata":{"id":"QHUmwpbVW6rL","outputId":"e77151db-cd5e-40de-d455-21ecd7b58838"},"source":["aggregatedDowDF = (joinedDowDF\n","  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n","  .sum(\"requests\")                                             \n","  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n","  .orderBy(col(\"dow\"))\n",")\n","# Display and then graph...\n","aggregatedDowDF.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---+---------+-----------+---------+----------+\n","|dow| longName|abbreviated|shortName|  Requests|\n","+---+---------+-----------+---------+----------+\n","|  1|   Monday|        Mon|        M|2356818845|\n","|  2|  Tuesday|        Tue|       Tu|1995034884|\n","|  3|Wednesday|        Wed|        W|1977615396|\n","|  4| Thursday|        Thr|       Th|1931508977|\n","|  5|   Friday|        Fri|        F|1842512718|\n","|  6| Saturday|        Sat|       Sa|1662762048|\n","|  7|   Sunday|        Sun|       Su|1576726066|\n","+---+---------+-----------+---------+----------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kL4e5GOJW6rM"},"source":["## Already Broadcasted\n","\n","You beleive or not, that was the broadcast join.\n","\n","The proof of which can be seen by looking at the physical plan.\n","\n","Run the `explain()` and then look for **BroadcastHashJoin** and/or **BroadcastExchange**."]},{"cell_type":"code","metadata":{"id":"gGEOvBMJW6rM","outputId":"7a9d3bbd-f0f1-4912-f03d-a5a6981376f6"},"source":["aggregatedDowDF.explain()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["== Physical Plan ==\n","*(4) Sort [dow#122 ASC NULLS FIRST], true, 0\n","+- Exchange rangepartitioning(dow#122 ASC NULLS FIRST, 8)\n","   +- *(3) HashAggregate(keys=[dow#122, longName#123, abbreviated#124, shortName#125], functions=[sum(cast(requests#2 as bigint))])\n","      +- Exchange hashpartitioning(dow#122, longName#123, abbreviated#124, shortName#125, 8)\n","         +- *(2) HashAggregate(keys=[dow#122, longName#123, abbreviated#124, shortName#125], functions=[partial_sum(cast(requests#2 as bigint))])\n","            +- *(2) Project [requests#2, dow#122, longName#123, abbreviated#124, shortName#125]\n","               +- *(2) BroadcastHashJoin [cast(dow#53 as int)], [dow#122], Inner, BuildRight\n","                  :- *(2) Filter isnotnull(dow#53)\n","                  :  +- InMemoryTableScan [requests#2, dow#53], [isnotnull(dow#53)]\n","                  :        +- InMemoryRelation [capturedAt#10, site#1, requests#2, dow#53], StorageLevel(disk, memory, deserialized, 1 replicas)\n","                  :              +- *(1) Project [capturedAt#10, site#1, requests#2, date_format(capturedAt#10, u, Some(UTC)) AS dow#53]\n","                  :                 +- InMemoryTableScan [capturedAt#10, requests#2, site#1]\n","                  :                       +- InMemoryRelation [capturedAt#10, site#1, requests#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n","                  :                             +- *(3) Sort [capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST], true, 0\n","                  :                                +- Exchange rangepartitioning(capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST, 8)\n","                  :                                   +- *(2) Project [cast(unix_timestamp(timestamp#0, yyyy-MM-dd'T'HH:mm:ss, Some(UTC)) as timestamp) AS capturedAt#10, site#1, requests#2]\n","                  :                                      +- Exchange RoundRobinPartitioning(8)\n","                  :                                         +- *(1) FileScan parquet [timestamp#0,site#1,requests#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://datacouch.training.io:8020/user/training/data/pageviews_by_second.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:string,site:string,requests:int>\n","                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n","                     +- *(1) Project [dow#122, longName#123, abbreviated#124, shortName#125]\n","                        +- *(1) Filter isnotnull(dow#122)\n","                           +- *(1) FileScan parquet [dow#122,longName#123,abbreviated#124,shortName#125] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://datacouch.training.io:8020/user/training/data/day-of-week], PartitionFilters: [], PushedFilters: [IsNotNull(dow)], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YKXOAyyuW6rM"},"source":["From the code perspective, it just looks like other joins.\n","\n","So what's the difference between a regular and a broadcast-join?"]},{"cell_type":"markdown","metadata":{"id":"voH8c7v3W6rN"},"source":["## Standard Join\n","\n",">* In a standard join, **ALL** of the data is shuffled\n",">* This can be really expensive\n","<br/><br/>\n","<p>Here we can see, how all the records keyed by \"green\" are moved to the same partition.<br/>The process would be repeated for \"red\" and \"blue\" records.</p>"]},{"cell_type":"markdown","metadata":{"id":"KnVKhuAoW6rN"},"source":["## Broadcast Join\n",">* In the Broadcast Join, only the \"small\" data is moved.\n",">* It duplicates the \"small\" data across all executors.\n",">* But the \"big\" data is left untouched.\n",">* If the \"small\" data is small enough, this can be **VERY** efficient.\n","<br/><br/>\n","<p>Here we see the records keyed by \"red\" being replicated into the first partition.<br/>\n","   The process would be repeated for each executor.<br/>\n","   The entire process would be repeated again for \"green\" and \"blue\" records.</p>"]},{"cell_type":"markdown","metadata":{"id":"CWyVEswkW6rN"},"source":["## Broadcasted, How?\n","\n","Behind the scenes, Spark is analyzing our two `DataFrames`.\n","\n","It attempts to estimate if either or both are < 10MB.\n","\n","We can see/change this threshold value with the config **spark.sql.autoBroadcastJoinThreshold**. \n","\n","The documentation reads as follows:\n","> Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled..."]},{"cell_type":"code","metadata":{"id":"s0Ou_2DeW6rN","outputId":"eff8e8aa-a8ea-4121-8d34-93d476e148c4"},"source":["threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n","print(\"Threshold: {0:,}\".format( int(threshold) ))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Threshold: 10,485,760\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eJz6F6eLW6rP"},"source":["For this kind of cases, it will take the small `DataFrame`, the `labelsDF` in our case\n",">* Send the entire `DataFrame` to every **Executor**\n",">* Then do a join on the local copy of `labelsDF`\n",">* Compared to taking our big `DataFrame` `pageviewsWithDowDF` and shuffling it across all executors."]},{"cell_type":"markdown","metadata":{"id":"z-cNAfQZW6rQ"},"source":["We can see proof of this by dropping the threshold:"]},{"cell_type":"code","metadata":{"id":"hyzZGaZYW6rQ"},"source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XaRCdCnjW6rR"},"source":["Run the `explain()` and take a look for the **ABSENCE OF** the **BroadcastHashJoin** and/or **BroadcastExchange**."]},{"cell_type":"code","metadata":{"id":"IqRtZjhqW6rR","outputId":"c8d343b2-3f72-4c4d-ab9a-8acdb31cb017"},"source":["(joinedDowDF\n","  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n","  .sum(\"requests\")                                             \n","  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n","  .orderBy(col(\"dow\"))\n","  .explain()\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["== Physical Plan ==\n","*(6) Sort [dow#122 ASC NULLS FIRST], true, 0\n","+- Exchange rangepartitioning(dow#122 ASC NULLS FIRST, 8)\n","   +- *(5) HashAggregate(keys=[dow#122, longName#123, abbreviated#124, shortName#125], functions=[sum(cast(requests#2 as bigint))])\n","      +- *(5) HashAggregate(keys=[dow#122, longName#123, abbreviated#124, shortName#125], functions=[partial_sum(cast(requests#2 as bigint))])\n","         +- *(5) Project [requests#2, dow#122, longName#123, abbreviated#124, shortName#125]\n","            +- *(5) SortMergeJoin [cast(dow#53 as int)], [dow#122], Inner\n","               :- *(2) Sort [cast(dow#53 as int) ASC NULLS FIRST], false, 0\n","               :  +- Exchange hashpartitioning(cast(dow#53 as int), 8)\n","               :     +- *(1) Filter isnotnull(dow#53)\n","               :        +- InMemoryTableScan [requests#2, dow#53], [isnotnull(dow#53)]\n","               :              +- InMemoryRelation [capturedAt#10, site#1, requests#2, dow#53], StorageLevel(disk, memory, deserialized, 1 replicas)\n","               :                    +- *(1) Project [capturedAt#10, site#1, requests#2, date_format(capturedAt#10, u, Some(UTC)) AS dow#53]\n","               :                       +- InMemoryTableScan [capturedAt#10, requests#2, site#1]\n","               :                             +- InMemoryRelation [capturedAt#10, site#1, requests#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n","               :                                   +- *(3) Sort [capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST], true, 0\n","               :                                      +- Exchange rangepartitioning(capturedAt#10 ASC NULLS FIRST, site#1 ASC NULLS FIRST, 8)\n","               :                                         +- *(2) Project [cast(unix_timestamp(timestamp#0, yyyy-MM-dd'T'HH:mm:ss, Some(UTC)) as timestamp) AS capturedAt#10, site#1, requests#2]\n","               :                                            +- Exchange RoundRobinPartitioning(8)\n","               :                                               +- *(1) FileScan parquet [timestamp#0,site#1,requests#2] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://datacouch.training.io:8020/user/training/data/pageviews_by_second.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:string,site:string,requests:int>\n","               +- *(4) Sort [dow#122 ASC NULLS FIRST], false, 0\n","                  +- Exchange hashpartitioning(dow#122, 8)\n","                     +- *(3) Project [dow#122, longName#123, abbreviated#124, shortName#125]\n","                        +- *(3) Filter isnotnull(dow#122)\n","                           +- *(3) FileScan parquet [dow#122,longName#123,abbreviated#124,shortName#125] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://datacouch.training.io:8020/user/training/data/day-of-week], PartitionFilters: [], PushedFilters: [IsNotNull(dow)], ReadSchema: struct<dow:int,longName:string,abbreviated:string,shortName:string>\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z4q4tpFiW6rR"},"source":["And now that we are done, let's restore the original threshold:"]},{"cell_type":"code","metadata":{"id":"m2CxRSljW6rR"},"source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aU9_EFW7W6rS"},"source":["## broadcast()"]},{"cell_type":"markdown","metadata":{"id":"G9QMwC9KW6rS"},"source":["What if we wanted to broadcast the data and it was over the 10MB [default] threshold?\n","\n","Now, we can specify that a `DataFrame` is to be broadcasted by using the `broadcast()` operation from the `sql.functions` package.\n","\n","However, **it is only a hint**. Spark is allowed to ignore it."]},{"cell_type":"code","metadata":{"id":"IEB7I7X0W6rS","outputId":"2cbc50fe-5f12-4a42-9009-411eb97591be"},"source":["pageviewsWithDowDF.join(broadcast(labelsDF), pageviewsWithDowDF[\"dow\"] == labelsDF[\"dow\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[capturedAt: timestamp, site: string, requests: int, dow: string, dow: int, longName: string, abbreviated: string, shortName: string]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"SQXIEDnsW6rT"},"source":["## End of Exercise"]}]}