{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc147263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f73ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('lineage').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979fe384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.option(\"multiLine\",\"true\").json(\"D://TechnoAvengers//Sparkp-New//movies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c921349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- genre: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d044071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df2 = df1.filter(\"rating >4\")\n",
    "df3 = df2.filter(col(\"description\").contains(\"romantic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c01f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(1) MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[13] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.rdd.toDebugString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dca15daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter Contains('description, romantic)\n",
      "+- Filter (rating#4 > cast(4 as double))\n",
      "   +- Relation[_id#0,description#1,genre#2,name#3,rating#4,year#5L] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "_id: string, description: string, genre: array<string>, name: string, rating: double, year: bigint\n",
      "Filter Contains(description#1, romantic)\n",
      "+- Filter (rating#4 > cast(4 as double))\n",
      "   +- Relation[_id#0,description#1,genre#2,name#3,rating#4,year#5L] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(rating#4) AND isnotnull(description#1)) AND (rating#4 > 4.0)) AND Contains(description#1, romantic))\n",
      "+- Relation[_id#0,description#1,genre#2,name#3,rating#4,year#5L] json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [_id#0, description#1, genre#2, name#3, rating#4, year#5L]\n",
      "+- *(1) Filter (((isnotnull(rating#4) AND isnotnull(description#1)) AND (rating#4 > 4.0)) AND Contains(description#1, romantic))\n",
      "   +- FileScan json [_id#0,description#1,genre#2,name#3,rating#4,year#5L] Batched: false, DataFilters: [isnotnull(rating#4), isnotnull(description#1), (rating#4 > 4.0), Contains(description#1, romantic)], Format: JSON, Location: InMemoryFileIndex[file:/D:/TechnoAvengers/Sparkp-New/movies.json], PartitionFilters: [], PushedFilters: [IsNotNull(rating), IsNotNull(description), GreaterThan(rating,4.0), StringContains(description,r..., ReadSchema: struct<_id:string,description:string,genre:array<string>,name:string,rating:double,year:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7b11c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 (maxMethodCodeSize:607; maxConstantPoolSize:174(0.27% used); numInnerClasses:0) ==\n",
      "*(1) Project [_id#0, description#1, genre#2, name#3, rating#4, year#5L]\n",
      "+- *(1) Filter (((isnotnull(rating#4) AND isnotnull(description#1)) AND (rating#4 > 4.0)) AND Contains(description#1, romantic))\n",
      "   +- FileScan json [_id#0,description#1,genre#2,name#3,rating#4,year#5L] Batched: false, DataFilters: [isnotnull(rating#4), isnotnull(description#1), (rating#4 > 4.0), Contains(description#1, romantic)], Format: JSON, Location: InMemoryFileIndex[file:/D:/TechnoAvengers/Sparkp-New/movies.json], PartitionFilters: [], PushedFilters: [IsNotNull(rating), IsNotNull(description), GreaterThan(rating,4.0), StringContains(description,r..., ReadSchema: struct<_id:string,description:string,genre:array<string>,name:string,rating:double,year:bigint>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] filter_mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];\n",
      "/* 012 */\n",
      "/* 013 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 014 */     this.references = references;\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 018 */     partitionIndex = index;\n",
      "/* 019 */     this.inputs = inputs;\n",
      "/* 020 */     inputadapter_input_0 = inputs[0];\n",
      "/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 128);\n",
      "/* 022 */     filter_mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(filter_mutableStateArray_0[0], 8);\n",
      "/* 023 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 128);\n",
      "/* 024 */     filter_mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(filter_mutableStateArray_0[1], 8);\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   protected void processNext() throws java.io.IOException {\n",
      "/* 029 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 030 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 031 */\n",
      "/* 032 */       do {\n",
      "/* 033 */         boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n",
      "/* 034 */         double inputadapter_value_4 = inputadapter_isNull_4 ?\n",
      "/* 035 */         -1.0 : (inputadapter_row_0.getDouble(4));\n",
      "/* 036 */\n",
      "/* 037 */         boolean filter_value_2 = !inputadapter_isNull_4;\n",
      "/* 038 */         if (!filter_value_2) continue;\n",
      "/* 039 */\n",
      "/* 040 */         boolean filter_value_3 = false;\n",
      "/* 041 */         filter_value_3 = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(inputadapter_value_4, 4.0D) > 0;\n",
      "/* 042 */         if (!filter_value_3) continue;\n",
      "/* 043 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 044 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 045 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 046 */\n",
      "/* 047 */         boolean filter_value_8 = !inputadapter_isNull_1;\n",
      "/* 048 */         if (!filter_value_8) continue;\n",
      "/* 049 */\n",
      "/* 050 */         boolean filter_value_9 = false;\n",
      "/* 051 */         filter_value_9 = (inputadapter_value_1).contains(((UTF8String) references[1] /* literal */));\n",
      "/* 052 */         if (!filter_value_9) continue;\n",
      "/* 053 */\n",
      "/* 054 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 055 */\n",
      "/* 056 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 057 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 058 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 059 */         boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 060 */         ArrayData inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 061 */         null : (inputadapter_row_0.getArray(2));\n",
      "/* 062 */         boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 063 */         UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 064 */         null : (inputadapter_row_0.getUTF8String(3));\n",
      "/* 065 */         boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n",
      "/* 066 */         long inputadapter_value_5 = inputadapter_isNull_5 ?\n",
      "/* 067 */         -1L : (inputadapter_row_0.getLong(5));\n",
      "/* 068 */         filter_mutableStateArray_0[1].reset();\n",
      "/* 069 */\n",
      "/* 070 */         filter_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 071 */\n",
      "/* 072 */         if (inputadapter_isNull_0) {\n",
      "/* 073 */           filter_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 074 */         } else {\n",
      "/* 075 */           filter_mutableStateArray_0[1].write(0, inputadapter_value_0);\n",
      "/* 076 */         }\n",
      "/* 077 */\n",
      "/* 078 */         if (false) {\n",
      "/* 079 */           filter_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 080 */         } else {\n",
      "/* 081 */           filter_mutableStateArray_0[1].write(1, inputadapter_value_1);\n",
      "/* 082 */         }\n",
      "/* 083 */\n",
      "/* 084 */         if (inputadapter_isNull_2) {\n",
      "/* 085 */           filter_mutableStateArray_0[1].setNullAt(2);\n",
      "/* 086 */         } else {\n",
      "/* 087 */           // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 088 */           // written later.\n",
      "/* 089 */           final int project_previousCursor_0 = filter_mutableStateArray_0[1].cursor();\n",
      "/* 090 */\n",
      "/* 091 */           final ArrayData project_tmpInput_0 = inputadapter_value_2;\n",
      "/* 092 */           if (project_tmpInput_0 instanceof UnsafeArrayData) {\n",
      "/* 093 */             filter_mutableStateArray_0[1].write((UnsafeArrayData) project_tmpInput_0);\n",
      "/* 094 */           } else {\n",
      "/* 095 */             final int project_numElements_0 = project_tmpInput_0.numElements();\n",
      "/* 096 */             filter_mutableStateArray_1[1].initialize(project_numElements_0);\n",
      "/* 097 */\n",
      "/* 098 */             for (int project_index_0 = 0; project_index_0 < project_numElements_0; project_index_0++) {\n",
      "/* 099 */               if (project_tmpInput_0.isNullAt(project_index_0)) {\n",
      "/* 100 */                 filter_mutableStateArray_1[1].setNull8Bytes(project_index_0);\n",
      "/* 101 */               } else {\n",
      "/* 102 */                 filter_mutableStateArray_1[1].write(project_index_0, project_tmpInput_0.getUTF8String(project_index_0));\n",
      "/* 103 */               }\n",
      "/* 104 */\n",
      "/* 105 */             }\n",
      "/* 106 */           }\n",
      "/* 107 */\n",
      "/* 108 */           filter_mutableStateArray_0[1].setOffsetAndSizeFromPreviousCursor(2, project_previousCursor_0);\n",
      "/* 109 */         }\n",
      "/* 110 */\n",
      "/* 111 */         if (inputadapter_isNull_3) {\n",
      "/* 112 */           filter_mutableStateArray_0[1].setNullAt(3);\n",
      "/* 113 */         } else {\n",
      "/* 114 */           filter_mutableStateArray_0[1].write(3, inputadapter_value_3);\n",
      "/* 115 */         }\n",
      "/* 116 */\n",
      "/* 117 */         if (false) {\n",
      "/* 118 */           filter_mutableStateArray_0[1].setNullAt(4);\n",
      "/* 119 */         } else {\n",
      "/* 120 */           filter_mutableStateArray_0[1].write(4, inputadapter_value_4);\n",
      "/* 121 */         }\n",
      "/* 122 */\n",
      "/* 123 */         if (inputadapter_isNull_5) {\n",
      "/* 124 */           filter_mutableStateArray_0[1].setNullAt(5);\n",
      "/* 125 */         } else {\n",
      "/* 126 */           filter_mutableStateArray_0[1].write(5, inputadapter_value_5);\n",
      "/* 127 */         }\n",
      "/* 128 */         append((filter_mutableStateArray_0[1].getRow()));\n",
      "/* 129 */\n",
      "/* 130 */       } while(false);\n",
      "/* 131 */       if (shouldStop()) return;\n",
      "/* 132 */     }\n",
      "/* 133 */   }\n",
      "/* 134 */\n",
      "/* 135 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.explain(mode=\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e20cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (3)\n",
      "+- * Filter (2)\n",
      "   +- Scan json  (1)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [6]: [_id#0, description#1, genre#2, name#3, rating#4, year#5L]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/D:/TechnoAvengers/Sparkp-New/movies.json]\n",
      "PushedFilters: [IsNotNull(rating), IsNotNull(description), GreaterThan(rating,4.0), StringContains(description,romantic)]\n",
      "ReadSchema: struct<_id:string,description:string,genre:array<string>,name:string,rating:double,year:bigint>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [6]: [_id#0, description#1, genre#2, name#3, rating#4, year#5L]\n",
      "Condition : (((isnotnull(rating#4) AND isnotnull(description#1)) AND (rating#4 > 4.0)) AND Contains(description#1, romantic))\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [6]: [_id#0, description#1, genre#2, name#3, rating#4, year#5L]\n",
      "Input [6]: [_id#0, description#1, genre#2, name#3, rating#4, year#5L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633b24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
