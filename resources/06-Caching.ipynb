{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 06-Caching.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"name":"Caching","notebookId":9851},"cells":[{"cell_type":"markdown","metadata":{"id":"Gur8IAfvXOSS"},"source":["# Caching\n","\n","**Technical Accomplishments:**\n","* Understaning for How caching works?\n","* Explore the various caching mechanisims\n","* Discuss tips for the best use of the cache"]},{"cell_type":"markdown","metadata":{"id":"-1Z9K3kDXOSa"},"source":["## A Fresh Start\n","For this section, first of all there is need to clear the existing cache.\n","\n","There are several ways to accomplish this:\n","  * Remove each cache one-by-one which is fairly problematic\n","  * Restart the cluster - takes a fair while to come back online\n","  * Just blow the entire cache away - this will affect each and every user on the cluster!!"]},{"cell_type":"code","metadata":{"id":"og0ng1JIXOSc"},"source":["#!!! DO NOT RUN THIS ON A SHARED CLUSTER !!!\n","\n","# spark.catalog.clearCache()\n","\n","#!!! It will Delete the cache of your system and Your's Co-Worker's !!!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvh38ezvXOSc"},"source":["This will ensure that any caches produced by other exercises will be removed.\n","\n","Next, open the **Spark UI** and go to the **Storage** tab - it should be empty."]},{"cell_type":"markdown","metadata":{"id":"pnTWghoiXOSd"},"source":["## The Data Source\n","\n","This data uses the **Pageviews By Seconds** data set.\n","\n","The parquet files are located in the HDFS at **data/pageviews_by_second.parquet**."]},{"cell_type":"code","metadata":{"id":"X1YeRi0sXOSd"},"source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","\n","schema = StructType(\n","  [\n","    StructField(\"timestamp\", StringType(), False),\n","    StructField(\"site\", StringType(), False),\n","    StructField(\"requests\", IntegerType(), False)\n","  ]\n",")\n","\n","fileName = \"/home/Downloads/data/pageviews_by_second.tsv\"\n","\n","pageviewsDF = (spark.read\n","  .option(\"header\", \"true\")\n","  .option(\"sep\", \"\\t\")\n","  .schema(schema)\n","  .csv(fileName)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojCv5SLXXOSe"},"source":["The 255 MB pageviews data is currently in HDFS, which means each time you scan through it, your Spark cluster has to read the 255 MB of data remotely over the network."]},{"cell_type":"markdown","metadata":{"id":"EYUgS2fEXOSe"},"source":["Once again, use the `count()` action to scan the entire 255 MB file from disk and count how many total rows there are in dataset:"]},{"cell_type":"code","metadata":{"id":"EMzi_KsJXOSf","outputId":"a839dd06-835b-4ca5-a9d9-975a77231c1e"},"source":["total = pageviewsDF.count()\n","\n","print(\"Record Count: {0:,}\".format( total ))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Record Count: 7,200,000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_DJC2r0JXOSg"},"source":["The pageviews DataFrame contains 7.2 million rows.\n","\n","Do Make a note of how long the previous operation takes.\n","\n","Re-run it several times so as trying to establish an average.\n","\n","Now Let's try a slightly more complicated operation, such as sorting, which induces an \"expensive\" shuffle."]},{"cell_type":"code","metadata":{"id":"sIJmAeaUXOSg","outputId":"c5657459-e060-48aa-f2e6-c9cca2b13675"},"source":["(pageviewsDF\n","  .orderBy(\"requests\")\n","  .count()\n",")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7200000"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"66_rqz9NXOSh"},"source":["Again, do make a note of how long the operation takes.\n","\n","Rerun it several times to get an average time."]},{"cell_type":"markdown","metadata":{"id":"cqRxmRW8XOSh"},"source":["Each and Every time we re-run these operations, it goes all the way back to the original data store.\n","\n","This requires pulling all the data across the network for every execution.\n","\n","In most of the cases, this network IO is the most expensive part of a job."]},{"cell_type":"markdown","metadata":{"id":"87cI0aNVXOSi"},"source":["## cache()\n","\n","We can avoid all of this overhead by caching the data on the executors.\n","\n","Just go ahead and run the following command.\n","\n","Don't forget to make a note of how long it takes to execute."]},{"cell_type":"code","metadata":{"id":"1mRqKn2mXOSi","outputId":"b8575689-e7b2-44f1-9e9b-2309efec17aa"},"source":["pageviewsDF.cache()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[timestamp: string, site: string, requests: int]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"L3p5_yvBXOSi"},"source":["The `cache(..)` operation doesn't perform any rocket science but it only mark a `DataFrame` as cacheable.\n","\n","And while it does return an instance of `DataFrame` it is not technically a transformation or action\n","\n","In order to actually cache the data, Spark has to process over each and every single record.\n","\n","As Spark processes every record, the cache will be materialized.\n","\n","A very common method for materializing the cache is to execute a `count()`.\n","\n","**BUT BEFORE YOU DO** Check the **Spark UI** to make sure it's still empty even after calling `cache()`."]},{"cell_type":"code","metadata":{"id":"w-4RDhcWXOSj","outputId":"e2c766ad-c317-4d62-935f-72ef1c6fed73"},"source":["pageviewsDF.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7200000"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"dK-JEo0_XOSj"},"source":["The last `count()` will take a little longer than normal.\n","\n","It has to perform the cache and do the work of materializing the cache.\n","\n","Now the `pageviewsDF` is cached **AND** the cache has been materialized.\n","\n","Before we rerun our queries, check the **Spark UI** and the **Storage** tab.\n","\n","Now, run the two queries and compare their execution time to the ones above."]},{"cell_type":"code","metadata":{"id":"k3r5tOblXOSj","outputId":"7adedff9-551f-4e99-af14-a414ed3f7cd1"},"source":["pageviewsDF.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7200000"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"sK6qR317XOSk","outputId":"f17cc214-b8ca-4850-a7dd-f253aa13612d"},"source":["(pageviewsDF\n","  .orderBy(\"requests\")\n","  .count()\n",")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7200000"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"QYATdlxBXOSk"},"source":["Was it Faster?\n","\n","All of our data is being stored in cache on the executors.\n","\n","We are no longer making network calls. Our plain `count()` should be sub-second. Our `orderBy()` & `count()` should be around 3-4 seconds."]},{"cell_type":"markdown","metadata":{"id":"6CoAAL3pXOSk"},"source":["## Spark UI - Storage"]},{"cell_type":"markdown","metadata":{"id":"6fJKjZ1BXOSl"},"source":["Now that the pageviews `DataFrame` is cached in memory let's review the **Spark UI** in more detail.\n","\n","In the **RDDs** table, you should see only one record - multiple if you re-run the `cache()` operation."]},{"cell_type":"markdown","metadata":{"id":"PURwF_glXOSl"},"source":["Let's review the **Spark UI**'s **Storage** in detail:\n","\n","* RDD Name\n","* Storage Level\n","* Cached Partitions\n","* Fraction Cached\n","* Size in Memory\n","* Size on Disk"]},{"cell_type":"markdown","metadata":{"id":"9zTvY9ycXOSl"},"source":["Now, dig deeper into the storage details.\n","\n","Click on the link provided in the **RDD Name** column to open the **RDD Storage Info**."]},{"cell_type":"markdown","metadata":{"id":"bfdmrad2XOSl"},"source":["Review the **RDD Storage Info**:\n","\n","* Size in Memory\n","* Size on Disk\n","* Executors\n","\n","If you recall:\n","\n","* We should have 8 partitions.\n","* With 255MB of data divided into 8 partitions.\n","* The first seven partitions should be 32MB each.\n","* The last partition will be significantly smaller than the others.\n","\n","**Question:** Why is the **Size in Memory** nowhere near 32MB?\n","\n","**Question:** What is the difference between **Size in Memory** and **Size on Disk**?"]},{"cell_type":"markdown","metadata":{"id":"OxgjR-_hXOSm"},"source":["## persist()\n","\n","`cache()` is just an alias for the `persist()`\n","\n","Let's take a look at the API docs for:\n","\n","* `Dataset.persist()` - Scala\n","* `DataFrame.persist()` - Python\n","\n","`persist()` allows one to specify an additional parameter i.e. storage level, indicating how the data is cached:\n","\n","* DISK_ONLY\n","* DISK_ONLY_2\n","* MEMORY_AND_DISK\n","* MEMORY_AND_DISK_2\n","* MEMORY_AND_DISK_SER\n","* MEMORY_AND_DISK_SER_2\n","* MEMORY_ONLY\n","* MEMORY_ONLY_2\n","* MEMORY_ONLY_SER\n","* MEMORY_ONLY_SER_2\n","* OFF_HEAP\n","\n","** *Note:* ** *The default storage level for:*\n","* *RDDs are **MEMORY_ONLY**.*\n","* *DataFrames are **MEMORY_AND_DISK**.* \n","* *Streaming is **MEMORY_AND_DISK_2**.*"]},{"cell_type":"markdown","metadata":{"id":"q83yLJTMXOSm"},"source":["Before we can use the various storage levels, it's necessary to import the enumerations..."]},{"cell_type":"code","metadata":{"id":"O7aN38JTXOSm"},"source":["from pyspark import StorageLevel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QUtu2kcMXOSm"},"source":["**Question:** How do we purge data from the cache?\n","\n","`unpersist()` or `uncache()`?\n","\n","Want to Try it?"]},{"cell_type":"code","metadata":{"id":"BptcmMchXOSn"},"source":["# pageviewsDF.uncache()\n","# pageviewsDF.unpersist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcJxQk-cXOSo"},"source":["Real quick, go check the **Storage** tab in the **Spark UI** and confirm that the cache has been removed."]},{"cell_type":"markdown","metadata":{"id":"5jjZA2dxXOSo"},"source":["**Question:** What will happen if you take 75% of the cache and then I come along and try to use 50% (of the total) only\n","\n","* with **MEMORY_ONLY**?\n","* with **MEMORY_AND_DISK**?\n","* with **DISK_ONLY**?"]},{"cell_type":"markdown","metadata":{"id":"ZLQB9-U2XOSo"},"source":["## RDD Name\n","\n","If you haven't noticed yet, the **RDD Name** on the **Storage** tab in the **Spark UI** is a big ugly name.\n","\n","It's a bit hacky, but there is a workaround for assigning a name.\n","0. Create your `DataFrame`.\n","0. From that `DataFrame`, create a temporary view with any name.\n","0. Specifically, cache the table via the `SparkSession` and its `Catalog`.\n","0. Materialize the cache."]},{"cell_type":"code","metadata":{"id":"otGvy5e9XOSp","outputId":"79feb4a2-d639-4881-93f3-7085024ad4b4"},"source":["pageviewsDF.unpersist()\n","\n","pageviewsDF.createOrReplaceTempView(\"Pageviews_DF_Python\")\n","spark.catalog.cacheTable(\"Pageviews_DF_Python\")\n","\n","pageviewsDF.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7200000"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"WhqSl9tQXOSp"},"source":["And now to clean up after ourselves"]},{"cell_type":"code","metadata":{"id":"qaxRl-MvXOSq","outputId":"2e456ad1-2fdc-4c04-b568-76a418aceae3"},"source":["pageviewsDF.unpersist()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[timestamp: string, site: string, requests: int]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"-IUWExJoXOSq"},"source":["## In this lesson you learned about:\n"," - Analyzing the performance of caching RDDs w.r.t DataFrames\n"," - Comparing and contrasting the various storage level options"]},{"cell_type":"code","metadata":{"id":"AXsTKBDlXOSq"},"source":["df = spark.read.csv(\"/home/Downloads/data/people-with-header-10m.txt\", header=\"true\", sep=\":\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9qWQezqXOSr","outputId":"89a994ed-649e-404a-bb7d-0dc957d89635"},"source":["df.cache() # cache is a lazy operation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[id: string, firstName: string, middleName: string, lastName: string, gender: string, birthDate: string, ssn: string, salary: string]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jc3JTzoiXOSr"},"source":["Show causes an action so cache is materialized.."]},{"cell_type":"code","metadata":{"id":"PVt40VN0XOSr","outputId":"389646a2-72f7-4c60-8a2b-f1e12c1dd748"},"source":["df.count() # cache for whole dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000000"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"0Tdo3FfKXOSr"},"source":["Don't ignore data types. How big is the file compared to in-memory?"]},{"cell_type":"code","metadata":{"id":"X0hDBkIhXOSs","outputId":"d782e0e2-1d39-43df-8567-53668e528a12"},"source":["df.unpersist()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[id: string, firstName: string, middleName: string, lastName: string, gender: string, birthDate: string, ssn: string, salary: string]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"_zLRtnJYXOSs"},"source":["It's bigger in memory than on disk! Why? Due to Java string object storage.\n","\n","<img src=\"https://files.training.databricks.com/images/tuning/java-string.png\" alt=\"Java String Memory allocation\"/><br/>\n","\n","\n","- A regular 4 byte string would end up taking 48 bytes. \n","- The diagram shows how the 40 bytes are allocated and we also need to round up byte usage to be divisible of 8 due to JVM padding. \n","- This is a very bloated representation knowing that of these 48 bytes, we're actually after only 4. \n","\n","Let's try with `inferSchema` instead."]},{"cell_type":"code","metadata":{"id":"tUt_siw_XOSs","outputId":"09d19870-0a02-4b2f-f6b0-61dc03b0f48d"},"source":["df2 = spark.read.csv(\"/home/Downloads/data/people-with-header-10m.txt\", header=\"true\", inferSchema=\"true\", sep=\":\")\n","\n","df2.cache().count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000000"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"7LtsjnEUXOSs"},"source":["Only takes up ~230MB vs ~330MB..."]},{"cell_type":"code","metadata":{"id":"VhJDYk3LXOSt","outputId":"622d3219-e735-421b-f87f-5bc80d21a9cf"},"source":["df2.unpersist() # we have to unpersist here otherwise the next cell won't re-cache the same dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[id: int, firstName: string, middleName: string, lastName: string, gender: string, birthDate: timestamp, ssn: string, salary: int]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"l95gdCPXXOSt"},"source":["Let's use MEMORY_AND_DISK."]},{"cell_type":"code","metadata":{"id":"fidbe4bCXOSt","outputId":"322deb54-f973-43d9-847e-120e1f4c2b35"},"source":["from pyspark import StorageLevel\n","\n","df3 = spark.read.csv(\"/home/Downloads/data/people-with-header-10m.txt\", header=\"true\", inferSchema=\"true\", sep=\":\")\n","\n","df3.persist(StorageLevel.MEMORY_AND_DISK).count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000000"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"gM3oPJQQXOSt"},"source":["Now only ~336MB, almost half of what we started with on storage! Let's compare that to an RDD."]},{"cell_type":"code","metadata":{"id":"MBUZeNbKXOSu","outputId":"50085eef-f5c3-435b-fd4e-cfbb1542488d"},"source":["myRDD = df3.rdd\n","myRDD.setName(\"myRDD\").cache().count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000000"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"-bQWG0NMXOSu"},"source":["Wow! The RDD took up significantly less space. Let's unpersist both of them and see how we can cache DataFrames with cleaner names."]},{"cell_type":"code","metadata":{"id":"10B7SHIvXOSu"},"source":["df3.unpersist()\n","myRDD.unpersist()\n","\n","df.createOrReplaceTempView(\"df\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvNFqUyXXOSu","outputId":"ae2d7625-ce34-4ad1-a634-33cf8b2fc79b"},"source":["spark.sql(\"CACHE TABLE df\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"JCI3runEXOSv"},"source":["## End of Exercise"]}]}